
\section{情報量の概念}

情報とは何だろうか。
恐らくは受け手に何らかの変化を促す抽象的な存在であろうが、厳密な
定義は必ずしも容易ではないだろう。ただ我々は日々、天候、為替相場、
国会で可決された法律、社会的な事件、身近で起こった出来事など
様々な情報を入手している。これら様々な情報を数学的に取り扱うのが
情報理論である。情報理論では情報の内容というよりむしろ情報量を取り扱う。

それでは情報量とは何であろうか。以下の2つの情報があったとする。

\begin{enumerate}
\item 1月初旬の北海道に大雪が降った
\item 3月初旬の沖縄に大雪が降った
\end{enumerate}

\noindent
このうち、情報量が多いと感じられるのは恐らく後者の方であろう。つまり
確率が低い事象が起こったとき、得られる情報量は多いと感じられる。
情報理論ではある事象\(x\)が起こる確率\(P(x)\)とその事象が起こったときに得られる
情報量\(I\)の関係を以下のように定式化する。

\begin{equation}
I(x)=-\log_{2}P(x)
\label{eq_info_amount10}
\end{equation}

\noindent
\(I\)の単位はビット(bit)である。例えば確率\(\frac{1}{4}\)の事象が起こったときに
得られる情報量は2ビットということになる。また公正なサイコロを振って2の目が出たときに
得られる情報量は\(\log_{2}6\)ビットである。

情報量には加法的な性質がある。A君がサイコロを振って出た目が段階的に
知らされる場合を考えてみよう。まず出た目が奇数であると分かったとき、得られる
情報は1ビットである。次にこの状態で出た目が2であると知らされたときに得られる
情報量は\(\log_{2}3\)ビットである。初めから2であると知らされたときに得られる
情報量は\(\log_{2}6\)ビットであるが、これは実は1と\(\log_{2}3\)の和である。

これをもう少し定式化してみよう。事象\(x\)が起こったときの確率を\(P(x)\)、
\(y\)が起こったときの確率を\(P(y)\)とする。\(x,y\)が同時に起こる確率は
\(P(x,y)\)と表す。また\(x\)が起こっている状態でさらに\(y\)が起こる確率\(P(y|x)\)は、

\begin{equation}
P(y|x)=\frac{P(x,y)}{P(x)}
\label{eq_prod_prob104}
\end{equation}

\noindent
である。これは

\begin{equation}
P(x,y)=P(x)P(y|x)
\label{eq_prod_prob105}
\end{equation}

\noindent
と変形されるが、両辺の\(\log_{2}\)を計算すると、

\begin{equation}
-\log_{2}P(x,y)=-\log_{2}P(x) + -\log_{2}P(y|x)
\label{eq_info_sum121}
\end{equation}

\noindent
となる。ここで式\ref{eq_info_amount10}の情報量の表記を使えば、

\begin{equation}
I(x,y)=I(x)+I(y|x)
\label{eq_info_sum123}
\end{equation}

\noindent
となる。特に\(x \subset y\)の場合、すなわち、\(x\)が起これば必ず\(y\)も起こる場合は、
\(p(x,y)=p(y)\)より、

\begin{equation}
I(y)=I(x)+I(y|x)
\label{eq_info_sum125}
\end{equation}

\noindent
が成立する。これを先ほどの段階的にサイコロの出目が判明するときに当てはめると、
\(I(y)\)が出目が2であると最初から分かったときの情報量に該当し、これは
出目が奇数であると分かったときに得られる情報量\(I(x)\)と、
出目が奇数であると分かっている状態でさらに出目が2であると分かったときに
得られる情報量\(I(y|x)\)の和に等しい。

この情報の加法性は情報量を考える上で重要な性質だが、情報量を定義する上で
加法性を成立させるような関数は\(\log_{2}P(x)\)以外にはないのだろうか。
これを考察しよう。加法性の中核は式\ref{eq_prod_prob105}の右辺の積が
式\ref{eq_info_sum123}の和に変わるところである。つまり、事象\(x_{1}\)が
起こる確率\(p_{1}=P(x_{1})\)と事象\(x_{2}\)が起こる確率\(p_{2}=P(x_{2})\)に対して、
情報量\(f(p_{1})\)、\(f(p_{2})\)が定義されるとすれば、それは

\begin{equation}
f(p_{1}\cdot p_{2})=f(p_{1})+f(p_{2})
\label{eq_info_sum130}
\end{equation}

\noindent
を満たさなければならない。
このような性質を持つ\(f\)はどのような形になるだろうか。

式\ref{eq_info_sum130}で$p_{1}=1$を代入すると、
$f(p_{2})=f(1)+f(p_{2})$より、

\begin{equation}
f(1)=0
\label{eq_info_sum131}
\end{equation}

\noindent
となる。
次に式\ref{eq_info_sum130}を
$f(p_{1}p_{2})-f(p_{2})=f(p_{1})$と変形し、
\(p_{2}=p\)、\(p_{1}p_{2}=p+\Delta p\)とおけば、$p_{1}=(p+\Delta p)/p$だから、

\begin{equation}
f(p+\Delta p)-f(p)=f\left(1+\frac{\Delta p}{p}\right)
\end{equation}

\noindent
よって、

\begin{equation}
f^{\prime}(p)=\displaystyle \lim_{\Delta p\rightarrow 0}\frac{f(p+\Delta p)-f(p)}{\Delta p}=\lim_{\Delta p\rightarrow 0}\frac{f\left(1+\frac{\Delta p}{p}\right)}{\Delta p}
\label{eq_info_sum141}
\end{equation}

\noindent
ここで$\Delta p=p\Delta h$、$\lim_{\Delta h\rightarrow 0}\frac{f(1+\Delta h)}{\Delta h}=C$とおけば、
式\ref{eq_info_sum141}は
$\lim_{\Delta p\rightarrow 0}\frac{f\left(1+\frac{\Delta p}{p}\right)}{\Delta p}=\lim_{\Delta h\rightarrow 0}\frac{f(1+\Delta h)}{p\Delta h}=\frac{C}{p}$、 すなわち、

\begin{equation}
f^{\prime}(p)=\displaystyle \frac{C}{p}
\end{equation}

\noindent
と変形される。この微分方程式を$p$について解くと、

\begin{equation}
\int f^{\prime}(p)dp=\int\frac{C}{p}dp=C\log p+C_{*}=f(p)
\end{equation}

\noindent
$f(1)=0$より$C_{*}=0$だから、結局$f(p)=C\log p$の形になることが示された。

\section{対象となる事象に関する情報}

我々の周りでは天候の変化や為替レートの変化など常に様々な事象が起こり、そ
の情報が氾濫している。しかしその中には我々が特に知りたい情報というものが
存在するだろう。例えば、ドルを円で買いたい場合、円−ドルの為替レートの情
報が興味の対象となる。このとき、隣の県で雪が降ったという情報が入っても、
円−ドルの為替レートに関する情報にはほとんど関係がない。しかし、円がユーロに対して
高くなったという情報が入れば、ドルに対しても円が高くなったのではないかと推定
するのは自然であろう。このように注目している事象に関する情報を別の事象の情報から
ある程度得られることがある。本節では、注目している事象に関する情報がどれくらい
得られたかということを定式化する。

まずは観測しうる全ての事象を対象として、生起確率\(P(x)\)の事象\(x\)が
起こったとしよう。この場合、既に述べたように、得られる情報量\(I(x)\)は

\begin{equation}
I(x)=-\log_{2}P(x)
\label{eq_info_sum150}
\end{equation}

\noindent
である。次に既に事象\(y\)が起こったという情報が得られており、その上で
事象\(x\)が起こったとしよう。事象\(y\)が起こった状態でさらに事象\(x\)
が起こる確率は\(P(x|y)\)だから、この場合得られる情報量は、

\begin{equation}
I(x|y)=-\log_{2}P(x|y)
\label{eq_info_sum151}
\end{equation}

\noindent
となる。ここで情報の加法性より、
\(x\)が起こったときに得られる情報量(式\ref{eq_info_sum150})
=
\(y\)が起こったときに得られる\(x\)に関する情報量+
\(y\)が起こっている状態でさらに\(x\)が起こったときに得られる情報量
が成立する。従って\(y\)が起こったときに得られる\(x\)に関する情報量を
\(I_{x}(y)\)で表せば、

\begin{equation}
I_{x}(y)=I(x)-I(x|y)=\log_{2}\frac{P(x|y)}{P(x)}
\label{eq_info_sum152}
\end{equation}

\noindent
となる。

\vspace{1em}

\refstepcounter{preexer}

\noindent
{\bf 例題\thechapter-\thepreexer}:~ 2つのサイコロA,Bを振ることを考える。Aの出目が2であるという事象を\(x\)、Bの出目が
3であるという事象を\(y\)とする。\(y\)が起こったときに得られる情報量は\(I(y)=\log_{2}6\)であり、また
\(x\)が起こったことが判明している状態で、\(y\)が起こったときに得られる情報量も\(I(y|x)=\log_{2}6\)
である。従って、\(I_{y}(x)=I(y)-I(y|x)=0\)である。Aの出目の情報はBの出目を知る上で全く役に立たないことが
この結果から分かる。

\vspace{1em}

\refstepcounter{preexer}

\noindent
{\bf 例題\thechapter-\thepreexer}:~ 2つのサイコロA,Bを振ることを考える。AとBの出目の合計が11であるという情報を\(x\)、Bの出目が
5であるという情報を\(y\)とする。この場合も\(I(y)=\log_{2}6\)。一方、AとBの出目の合計が11のとき(Bの目は5または6しか有り得ない)、
Bの出目が5である確率は\(P(y|x)=\frac{1}{2}\)だから、\(I(y|x)=1\)となる。従って、
\(I_{y}(x)=\log_{2}6-1=\log_{2}3\)。この場合は、\(x\)の情報を得ることで\(y\)に関していくらかの情報を得ていることになる。

\vspace{1em}

\refstepcounter{preexer}

\noindent
{\bf 例題\thechapter-\thepreexer}:~ 10枚のクジがあって、そのうちの3枚が当たりだとする。A君、B君、C君がそれぞれ一枚ずつ
クジを引いた。この時点で\(A\)君が引いたクジが当たりだとする事象を\(x\)とすると、\(I(x)=\log_{2}\frac{10}{3}\)である。
これとは別に、\(A\)君のクジの中味が分からない状態で、B君、C君のクジが二人とも当たりだとする事象を\(y\)とする。
\(I(x|y)=\log_{2}{8}=3\)だから、\(I_{x}(y)=I(x)-I(x|y)=\log_{2}\frac{10}{3}-3=\log_{2}\frac{5}{12}<0\)となり、負の
値を取ることが分かる。この例から分かるように、ある事象が判明したときに、他の事象に関して得られる情報量が負の値を
取ることもある。この例をそのまま解釈すると、\(y\)が判明すると、
\(x\)に関する情報が得られるどころか、逆に失われるということになる。
情報が失われるというのは直感的には分かりにくいかもしれないが、情報量の
議論を展開するとこのような解釈が得られる。これはA君が当たりクジを引くのが難しくなったことに
相当すると考えるのが妥当であろう。

\section{平均情報量}

\subsection{エントロピー}
\label{subsec_entropy}

A君の住んでいるある国の地方では1年を通じて気候の変化がなく、正午の天候を\(X\)とすれば、
\(X\)が晴れ\((X=x_{1})\)、曇り\((X=x_{2})\)、雨\((X=x_{3})\)、雪\((X=x_{4})\)の確率が
それぞれ
\(P(X=x_{1})=0.5\)、
\(P(X=x_{2})=0.25\)、
\(P(X=x_{3})=0.25\)、
\(P(X=x_{4})=0\)であるとしよう。B君は遠い別の国に住んでおり、A君の住んでいる場所の天候はA君から
のみ知りうる。
仮にその日の天気が晴れだとA君より伝えられたとき、B君が得る情報量は1ビットになるし、曇りであれば2ビットの
情報量である。このように当然ながら、A君がB君にもたらす情報量は天候によって異なる。

ここで情報源としてのA君がB君に対して平均的にもたらす情報量を考えよう。
A君がもたらす情報量を確率変数\(X\)を使って表すと、\(-\log_{2}X\)となるが、ここで
節\ref{avvar1}の式\ref{eq_expect_103}(p.\pageref{eq_expect_103})を使うと、その期待値は、
\(-0.5\log_{2}0.5+
-0.25\log_{2}0.25+
-0.25\log_{2}0.25+
-0\log_{2}0\)である。\(0\log 0\)は一般的には定義されないが、\(\lim_{x\to 0}x\log_{a}x=0\)なので
(問題\ref{chap_cont_func}-\ref{exer_0log0}参照)、情報理論ではこれを0と定義する。

一般にある情報源\(X\)から情報\(x_{1},x_{2},\cdots,x_{n}\)のうちのいずれかが、それぞれ確率\(P(x_{1}),P(x_{2}),\cdots,P(x_{n})\)
(但し、\(\sum_{i=1}^{n}P_{i}=1)\))
で得られるとき、その平均情報量\(I(X)\)は、

\begin{equation}
I(X)=-\sum_{i=1}^{n}P(x_{i})\log_{2}P(x_{i})
\label{eq_entropy100}
\end{equation}

\noindent
で与えられる。この\(I(X)\)を\(X\)の{\bf エントロピー}\index{えんとろぴー@エントロピー}という。

\subsection{相互情報量}

\begin{table}
\begin{center}
\includegraphics[scale=0.8]{Figures/weather_sample1}
\end{center}
\vspace{-2em}
\caption{前日と当日の天気の組み合わせの確率}
\label{weather_sample1}
\end{table}

前小節の設定を少し変えてみよう。
表\ref{weather_sample1}に示すように、A君の住んでいるある国の地方の2月1日の正午の天候\(X\)を考え、
晴れ\((X=x_{1})\)、曇り\((X=x_{2})\)、雨\((X=x_{3})\)の確率が
それぞれ
\(P(X=x_{1})=2/5\)、
\(P(X=x_{2})=3/10\)、
\(P(X=x_{3})=1/5\)、
\(P(X=x_{4})=1/10\)
であるとしよう。これは過去の年の2月1日の天候データを集計して得られた統計モデルであるとする。
但し当日の天気は実は前日の天気\(Y\)の影響を受け、
前日が晴れだった場合(\(Y=y_{1}\))、
その確率は、
\(P(X=x_{1}|Y=y_{1})=8/13\)、
\(P(X=x_{2}|Y=y_{1})=3/13\)、
\(P(X=x_{3}|Y=y_{1})=1/13\)、
\(P(X=x_{4}|Y=y_{1})=1/13\)であるとしよう。
ここで情報の加法性を当日が晴れだった場合について考えると、
前日が晴れという事象を\(y_{1}\)で表せば、

\begin{equation}
I(x_{1})=I_{x_{1}}(y_{1})+I(x_{1}|y_{1})
\end{equation}

\noindent
が成立する。これを移項により変形すれば、

\begin{equation}
I_{x_{1}}(y_{1})=I(x_{1})-I(x_{1}|y_{1})
\end{equation}


\noindent
となる。\(I_{x_{1}}(y_{1})\)は前日が晴れだったときに、当日の天気が
晴れかどうかに関して得られる情報量であることに注意しよう。晴れの可能性が
高まれば高まるほど得られる情報も多くなる。
実際には\(y_{1}\)という通報を受けた段階で晴れの可能性
(\(P(X=x_{1}|Y=y_{1})\))
の他に、曇りの可能性(\(P(X=x_{2}|Y=y_{1})\))、
雨の可能性(\(P(X=x_{3}|Y=y_{1})\))、
雪の可能性(\(P(X=x_{4}|Y=y_{1})\))があるので、
これらの確率を掛けて平均情報量を計算すると、

\begin{equation}
\sum_{i=1}^{4}P(X=x_{i}|Y=y_{1})I(x_{i}|y_{1})
=
\sum_{i=1}^{4}P(X=x_{i}|Y=y_{1})I(x_{1})
-\sum_{i=1}^{4}P(X=x_{i}|Y=y_{1})I_{x_{i}}(y_{1})
\end{equation}

\noindent
が得られる。\(\sum_{i=1}^{4}P(X=x_{i}|Y=y_{1})I(x_{i}|y_{1})\)は
前日が晴れだったときに当日の天気に関して得られる平均情報量を
示している。これを{\bf 増加情報量}\index{ぞうかじょうほうりょう@増加情報量}
という。

これを少し一般化して議論しよう。
ある情報源\(X\)から情報\(x_{1},x_{2},\cdots,x_{n}\)のうちのいずれかが、それぞれ確率\(P(x_{1}),P(x_{2}),\cdots,P(x_{n})\)
(但し、\(\sum_{i=1}^{n}P(x_{i})=1)\))で得られるとする。次に条件\(y_{1}\)が成立するとその確率が
\(P(x_{1}|y_{1}),P(x_{2}|y_{1}),\cdots,P(x_{n}|y_{1})\)
(但し、\(\sum_{i=1}^{n}P(x_{i}|y_{1})=1\))に変化するとする。情報の加法性より、\(X=x_{i}\)のときに得られる情報量\(I(x_{i})\)は
\(y_{1}\)が成立することが判明したときに\(x_{i}\)に関して得られる情報量\(I_{x_{i}}(y_{1})\)と\(y_{1}\)が成立していることが分かっている状態で
\(X=x_{i}\)が判明したときに得られる情報量\(I(x_{i}|y_{1})\)の和で表されるから、

\begin{equation}
I(x_{i})=I_{x_{i}}(y_{1})+I(x_{i}|y_{1})
\end{equation}

\noindent
が成立する。移項による式の変形を行えば、

\begin{equation}
I_{x_{i}}(y_{1})
=I(x_{i})
-I(x_{i}|y_{1})
\end{equation}

\noindent
となる。これは\(y_{1}\)の通報があったとき、\(x_{i}\)に関して得られる情報量がどのようにして
計算されるかを表しているが、
実際には\(y_{1}\)の通報が得られるということは、\(x_{i} (i=1,2,\cdots,n)\)がそれぞれ
\(P(x_{1}|y_{1}),P(x_{2}|y_{1}),\cdots,P(x_{n}|y_{1})\)の確率で起こるので、
\(I_{x_{i}}(y_{1})\)の期待値(平均情報量)、すなわち増加情報量\(I_{X}(y_{1})\)を計算すると、
式\ref{eq_info_sum152}を使えば、

\begin{equation}
I_{X}(y_{1}) = \sum_{i=1}^{n}P(x_{i}|y_{1})I_{x_{i}}(y_{1})
= \sum_{i=1}^{n}P(x_{i}|y_{1}) \log_{2}\frac{P(x_{i}|y_{1})}{P(x_{i})}
\label{eq_relative_entropy101}
\end{equation}

\noindent
が得られる
% \footnote{期待値を計算するときに\(P(x_{i})\)ではなく、\(P(x_{i}|y_{1})\)を
% 掛けるのは妥当だろうか。そもそも情報の加法性の出発点は式\ref{eq_prod_prob105}の
% \(P(X \land y)=P(X)P(y|X)\)だったが、最後の部分を置き換えて、\(P(X \land y)=P(y)\cdot \frac{P(X \land y)}{P(y)}\)
% と表すことができる。これを情報量に直すと、\(I(X\land y)=I(y)+I(X \land y)-I(y)\)となる。
% 両辺ともに\(X \land y\)を含む項で式が構成されているため、
% 全ての\(X=x\)を考えるより、\(X \land y\)が成立するような\(X=x\)のみを考えるほうがが理に適っている
% と考えられる。そこで\(P(X=x_{i})\)ではなく、\(P(X=x_{i}|y_{1})=\frac{P(X=x_{i} \land y_{1})}{P(y_{1})}\)を
% 掛けて期待値を計算する。
% }
。

さて天候の例では、前日の天候\(Y\)が晴れ(\(Y=y_{1})\)のときのみを扱ったが、より一般的には前日が曇り(\(Y=y_{2}\))のときも
考えられるし、そのときの当日の各天気の確率\(
(P(X=x_{1}|Y=y_{2}),
P(X=x_{2}|Y=y_{2}),
P(X=x_{3}|Y=y_{2}),
P(X=x_{4}|Y=y_{2})
\))も前日が晴れのときとは別に考えることが可能である。

一般化した議論に戻ると、考えられる通報\(Y=y_{1},y_{2},\cdots,y_{m}\)に対して、増加情報量
\(I_{X}(y_{1}), I_{X}(y_{2}), \cdots, I_{X}(y_{m})\)を考えることができる。
通報\(Y=y_{1},y_{2},\cdots,y_{m}\)を受ける確率がそれぞれ\(
P(Y=y_{1}),
P(Y=y_{2}),\cdots,
P(Y=y_{m})\)
であれば、増加情報量の期待値\(I_{X}(Y)\)すなわち、

\begin{equation}
I_{X}(Y)=\sum_{j=1}^{m}P(y_{j})I_{X}(y_{j})
\label{eq_mutual_info101}
\end{equation}

\noindent
を考えることが可能である。式\ref{eq_relative_entropy101}を式\ref{eq_mutual_info101}
に代入すれば、

\begin{equation}
I_{X}(Y)=\sum_{j=1}^{m}P(y_{j})\sum_{i=1}^{n}P(x_{i}|y_{i}) \log_{2}\frac{P(x_{i}|y_{i})}{P(x_{i})}
=\sum_{j=1}^{m}\sum_{i=1}^{n}P(y_{j})P(x_{i}|y_{i}) \log_{2}\frac{P(x_{i}|y_{i})}{P(x_{i})}
\label{eq_mutual_info102}
\end{equation}

\noindent
ここで、\(P(y_{j})P(x_{i}|y_{j})=P(x_{i},y_{j})\)を使えば、

\begin{equation}
I_{X}(Y)=\sum_{j=1}^{m}\sum_{i=1}^{n}P(x_{i},y_{j})\log_{2}\frac{P(x_{i},y_{j})}{P(x_{i})P(y_{j})}
\end{equation}

\noindent
を得る。ここで、

\begin{eqnarray}
I_{Y}(X) & = & \sum_{i=1}^{n}\sum_{j=1}^{m}P(Y=y_{j},X=x_{i})\log_{2}\frac{P(Y=y_{j},X=x_{i})}{P(Y=y_{j})P(X=x_{i})}\nonumber\\
         & = & \sum_{j=1}^{m}\sum_{i=1}^{n}P(X=x_{i},Y=y_{j})\log_{2}\frac{P(X=x_{i},Y=y_{j})}{P(X=x_{i})P(Y=y_{j})}\nonumber\\
         & = & I_{X}(Y)
\end{eqnarray}

\noindent
なので、通報\(X\)によって得られる\(Y\)に関する平均情報量と、通報\(Y\)によって得られる\(X\)に関する平均情報量
は等しいことが分かる。そこで\(I_{X}(Y)\)を\(X\)と\(Y\)の{\bf 相互情報量(mutual information)}\index{そうごじょうほうりょう@相互情報量}
といい、\(I(X;Y)=I_{X}(Y)=I_{Y}(X)\)で表す。



