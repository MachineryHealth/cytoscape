
\section{Frequency calculation}
\label{stdfreq_proof}

Suppose that we are dealing with \(n\) sequences to find a specific
pattern.  Let \(p\) be probability of observing the specific pattern
in one given sequence. The probability that one observe exactly \(x\) sequences with the specific pattern would be

\begin{equation}
P(x) = {n\choose x}p^{x}(1-p)^{x} 
\label{eq1}
\end{equation}

\noindent
where \(x\) is a random variable that denotes observed number of
sequences with the specific pattern. Now suppose the normalized value
\(z\) for \(x\) with the following formula:

\begin{equation}
z = \frac{(x - np)}{\sqrt{np(1-p)}} 
\end{equation}

\noindent
Moment generating function for \(z\) would be

\[
M_{z}(t) = 
\sum_{z}{n\choose z\sqrt{np(1-p)}+np}p^{z\sqrt{np(1-p)}+np}(1-p)^{n-(\sqrt{np(1-p)}+np)}e^{tz}
\]
\[
 = \sum_{x = 0}^{n}{n\choose x}p^{x}(1-p)^{n-x}e^{t(x-np)/\sqrt{np(1-p)}}
\]

\begin{equation}
 = e^{-\frac{np}{\sqrt{np(1-p)}}t}\sum_{x=0}^{n}{n\choose x}(pe^{\frac{t}{\sqrt{np(1-p)}}})^{x}(1-p)^{n-x}
\label{eq3}
\end{equation}

\noindent 
By applying equation \((\alpha + \beta)^n = \sum_{x=0}^{n}{n\choose x}\alpha^{x}\beta^{n-x}\), \ref{eq3} can be written

\begin{equation}
M_{z}(t) = e^{-\frac{np}{\sqrt{np(1-p)}}}t((1-p) + pe^{\frac{t}{\sqrt{np(1-p)}}})^{n}
\end{equation}

\noindent
Subsequently,

\begin{equation}
\log M_{z}(t) = -\frac{np}{\sqrt{np(1-p)}} + n\log(q + pe^{\frac{t}{\sqrt{np(1-p)}}}) = -\frac{np}{\sqrt{np(1-p)}}t + n\log(1+u)
\label{eq6}
\end{equation}

\vspace{1em}

\noindent
where \(u = p(-1 + e^{t/\sqrt{np(1-p)}})\). By expanding \(\log(1+u)\), we obtain

\begin{equation}
\log(1+u)=u- \frac{u^{2}}{2} + \frac{u^{3}}{3} - \cdots
\label{eq7}
\end{equation}

\noindent
Also by expanding \(e^{\frac{t}{\sqrt{np(1-p)}}}\) we obtain

\[
u=p(-1+e^{\frac{t}{\sqrt{np(1-p)}}})
\]
\begin{equation}
=p\left\{\left(\frac{t}{\sqrt{np(1-p)}} \right)
+ \frac{1}{2!}\left(\frac{t}{\sqrt{np(1-p)}}\right)^{2}
+ \frac{1}{3!}\left(\frac{t}{\sqrt{np(1-p)}}\right)^{3}
+ \cdots 
\right\}
\label{eq8}
\end{equation}

\noindent
By substituting \(u\) in \ref{eq7} with \ref{eq8},

\[ \log(1+u)=
p\left\{\underline{\left(\frac{t}{\sqrt{np(1-p)}} \right)
+ \frac{1}{2!}\left(\frac{t}{\sqrt{np(1-p)}}\right)^{2}}_{(1)}
+ \frac{1}{3!}\left(\frac{t}{\sqrt{np(1-p)}}\right)^{3}
+ \cdots 
\right\}
\]
\[-
\frac{1}{2}p^{2}\left\{\underline{\left(\frac{t}{\sqrt{np(1-p)}} \right)}_{(2)}
+ \frac{1}{2!}\left(\frac{t}{\sqrt{np(1-p)}}\right)^{2}
+ \frac{1}{3!}\left(\frac{t}{\sqrt{np(1-p)}}\right)^{3}
+ \cdots 
\right\}^{2}
\]
\[+
\frac{1}{3}p^{3}\left\{\left(\frac{t}{\sqrt{np(1-p)}} \right)
+ \frac{1}{2!}\left(\frac{t}{\sqrt{np(1-p)}}\right)^{2}
+ \frac{1}{3!}\left(\frac{t}{\sqrt{np(1-p)}}\right)^{3}
+ \cdots 
\right\}^{3}
\]
\[
\vdots
\]
\[ = p\times (1) - \frac{1}{2}p^{2}\times (2) 
   + \sum_{k=3}^{\infty}c_{k}\left(\frac{t}{\sqrt{np(1-p)}}\right)^{k}
\]
\begin{equation}
   = \frac{pt}{\sqrt{np(1-p)}} + \frac{1}{2n}t^{2} + \sum_{k=3}^{\infty}c_{k}\left(\frac{t}{\sqrt{np(1-p)}}\right)^{k}
\end{equation}

\noindent
and from \ref{eq6}, we obtain

\begin{equation}
\log M_{z}(t) = \frac{t^{2}}{2} + \sum_{k=3}^{\infty}c_{k}\left(\frac{t}{\sqrt{p(1-p)}}\right)^{k}%\frac{1}{\sqrt{n}^{k-2}}
\end{equation}

\noindent
where \(c_{k}\) (\(k \geq 3\)) are multiples that do not include \(n\). Thus we obtain \(\lim_{n\to\infty}\log M_{z}(t) = \frac{t^{2}}{2}\) and

\begin{equation}
\lim_{n\to\infty} M_{z}(t)= e^{\frac{t^{2}}{2}}
\label{eq_bino_mo}
\end{equation}

\noindent
Now, probability density function of normal distribution \(N(\mu, \sigma^{2})\) is

\begin{equation}
f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}
\end{equation}

\noindent
and its moment generating function is

\[
M_{x}^{N(\mu,\sigma^{2})}(t) = \int_{-\infty}^{\infty}e^{tx}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}
= \frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}\mbox{exp}
\left[
-\frac{1}{2\sigma^{2}}\left\{(x-\mu)^{2} - 2\sigma^{2}tx\right\}
\right]
dx
\]
\[
=\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}\mbox{exp}
\left[
- \frac{1}{2\sigma^{2}} 
    \left\{(x-(\mu+\sigma^{2}t)
    \right\}^{2} 
+ \mu t + \frac{\sigma^{2}}{2}t^{2}
\right]
dx
\]

\begin{equation}
=\mbox{exp}\left(\mu t + \frac{\sigma^{2}}{2}t^{2} \right) \cdot
\underline{
\frac{1}{\sqrt{2\pi}\sigma}
\int_{-\infty}^{\infty}\mbox{exp}
\left[
-\frac{1}{2\sigma^{2}}
   \left\{
      x-(\mu + \sigma^{2}t)
   \right\}^{2}
\right]dx}_{(3)}
\end{equation}

\noindent
Since (3) is integral of probability density distribution of 
\(N(\mu + \sigma^{2}t, \sigma^{2})\), (3) = 0. Subsequently,

\begin{equation}
M_{x}^{N(\mu,\sigma^{2})}(t) = \mbox{exp}\left(\mu t + \frac{\sigma^{2}}{2}t^{2} \right)
\label{eq_std_mo}
\end{equation}

According to \ref{eq_std_mo}, moment generating function for \(N(0,1)\) will be \(e^{\frac{t^{2}}{2}}\), which is identical to right hand of formula \ref{eq_bino_mo}.

\noindent
Since \(e^{\frac{t^{2}}{2}}\) is also moment generating function of normal distribution \(N(0,1)\), 

\[
P(a \leq x \leq b) = \sum_{a \leq x \leq b}{n\choose x}p^{x}(1-p)^{n-x}
\quad
\to 
\quad
\int_{(a-np)/\sqrt{np(1-p)}}^{(b-np)/\sqrt{np(1-p)}}
   \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^{2}}dz
\]
\begin{equation}
\mbox{as}\quad n \to \infty
\end{equation}

\noindent
Thus

\begin{equation}
\mbox{Z Score} = \frac{x - np}{\sqrt{np(1-p)}} \quad \sim \quad N(0,1)
\label{eq_sd}
\end{equation}




\section{Statistical test on difference of frequencies}

Let \(P^{ATG}_{SD}\) be probability of observing core SD(Shine-Dalgarno)
sequence in ATG start sequence. 
Let \(P^{GTG}_{SD}\) be probability of observing core SD
sequence in GTG start sequence. 

\vspace{1em}

\noindent
Null hypothesis is

\vspace{1em}

\(H_{0} : P^{ATG}_{SD} = P^{GTG}_{SD}\)

\vspace{1em} 

\noindent
and alternative hypothesis is

\vspace{1em}

\(H_{1} : P^{ATG}_{SD} \neq P^{GTG}_{SD}\)
 
\vspace{1em}

\noindent
To test these hypothesis, the following \(\chi^{2}\) statistics can be
used.

\vspace{1em}

\[
\chi^{2}_{1} = \frac{(O^{atg}_{SD} - E^{atg}_{SD})^{2}}{E^{atg}_{SD}}
 + \frac{(O^{gtg}_{SD} - E^{gtg}_{SD})^2}{E^{gtg}_{SD}} =
 \frac{ (N^{atg}_{SD} - \frac{N_{SD}}{N}N_{atg})^2 }{
   \frac{N_{SD}}{N}N_{atg}} + \frac{ (N^{gtg}_{SD} - \frac{N_{SD}}{N}N_{gtg})^2 }{
   \frac{N_{SD}}{N}N_{gtg}}\] 

\begin{equation}
=  \frac{ (N^{atg}_{SD} - \frac{N_{SD}}{N}N_{atg})^2 }{
   \frac{N_{SD}}{N}N_{atg}} + \frac{ (N_{SD} - N^{atg}_{SD} - 
\frac{N_{SD}}{N}(N - N_{atg}))^2 }{
   \frac{N_{SD}}{N}(N - N_{atg})}  
 = \frac{ (N^{atg}_{SD} - \frac{N_{atg}}{N}N_{SD})^2 }{ N_{SD}
 \frac{N_{atg}}{N}\frac{N - N_{atg}}{N}}
\end{equation}

\vspace{1em}

\noindent 
where

\[ O^{atg}_{SD} = \mbox{Observed number of ATG start sequences with SD} \]
\[ O^{gtg}_{SD} = \mbox{Observed number of GTG start sequences with SD} \]
\[ E^{atg}_{SD} = \mbox{Expected number of ATG start sequences with SD} \]
\[ E^{gtg}_{SD} = \mbox{Expected number of GTG start sequences with SD} \]

\vspace{1em}

\noindent
\(E^{atg}_{SD}\) and \(E^{gtg}_{SD}\) can be calculated by the following
formula.
 
\vspace{1em}

\[ E^{atg}_{SD} = \frac{N_{SD}}{N}N_{atg} \quad ,\quad
   E^{gtg}_{SD} = \frac{N_{SD}}{N}N_{gtg} \]

\vspace{1em}
\noindent
where 
\[ N = \mbox{Number of sequences altogether}\]
\[ N_{SD} = \mbox{Number of sequences with SD}\]
\[ N_{atg} = \mbox{Number of ATG start sequences}\]
\[ N_{gtg} = \mbox{Number of GTG start sequences}\]

\vspace{1em}

\noindent
By calculating root of the above formula, we can obtain values that
follow normal distribution. 

\begin{equation}
\sqrt{\chi^{2}_{1}}\:\: = \:\: \frac{ N^{atg}_{SD} - \frac{N_{atg}}{N}N_{SD}}{ \sqrt{N_{SD}
 \frac{N_{atg}}{N}\frac{N - N_{atg}}{N}}}\:\:\sim\:\:N(0, 1).
\label{eq_sdfreq}
\end{equation}

\noindent
By substituting \(N_{SD}^{atg}\) with \(x\), \(N_{SD}\) with \(n\) and 
\(\frac{N_{atg}}{N}\) in \ref{eq_sdfreq}, we notice that it is exactly same as
\ref{eq_sd}.



\section{Expected value of entropy}
\label{ic_detail}

We consider the sequence of mutually independent random variables, 
\(\xi_{1}, \xi_{2}, \cdots , \xi_{n}, \cdots\), each of which takes on the value \(E_{i}\) with probability \(p_{i}\) (\(0 \leq p_{i} < 1, i = 1,2,\cdots, s,
  \:\: \sum_{i=1}^{s}p_{i} = 1 \)).

\noindent
Entropy is defined by the following formula.

\begin{equation}
H = H(p_{1}, \cdots , p_{s}) = -\sum_{i=1}^{s}p_{i}\log_{2}p_{i} 
\label{entropy}
\end{equation}

\noindent
Since \(\lim_{p \to 0}p\log p = 0\), we define \(0\log 0 = 0\).
We can show that 
\[
0 \leq H \leq 2
\]

\noindent
The estimate of \ref{entropy}, \(\hat H\) from samples with size \(N\) is given by the following formula. 


\[
\hat H = H(\hat p_{1}, \cdots , \hat p_{s}) = -\sum_{i=1}^{s}\hat
p_{i}\log_{2}\hat p_{i} 
\]

\noindent
and its expected value is\cite{label1896}

\begin{equation}
 \mbox{E}(\hat H) = H - \frac{s-1}{2N}\log_{2}e + O\left(\frac{1}{N^{2}}\right)
\label{mean_entro}
\end{equation}

\noindent
where \(N\) is number of samples. To prove \ref{mean_entro}, we first redefine \(H\) as

\begin{equation}
H = H(p_{1}, \cdots , p_{s}) = -\sum_{i=1}^{s}p_{i}\ln p_{i} 
\label{entropy_e}
\end{equation}

\noindent
By using 

\[
f(b) = f(a) + \frac{f'(a)}{1!}(b-a) + \frac{f''(a)}{2!}(b-a)^{2} + \cdots
\cdots + \frac{f^{(n-1)}(a)}{(n-1)!}(b-a)^{n-1} + \frac{f^{(n)}\{a + \theta(b-a)^{n}\}}{n!}
\]
\[ 0 < \theta < 1\]

\noindent
and

\[ 
\frac{d(-p_{i}\ln p_{i})}{d p_{i}} = -1 - \ln p_{i} , \quad
\frac{d^{k}(-p_{i}\ln p_{i})}{dp_{i}^{k}} = (-1)^{k-1}(k-2)!p_{i}^{-k+1}
\]
\[ i = 1,2,\cdots , s \quad k \geq 2 \]

\noindent
we can show that

\begin{equation}
\hat p_{i}\ln \hat p_{i} = p_{i}\ln p_{i} - (\hat p_{i} - p_{i})(1 + \ln p_{i})
- \frac{1}{2} \cdot \frac{(\hat p_{i} - p_{i})^{2}}{p_{i}}
+ \frac{1}{6} \cdot \frac{(\hat p_{i} - p_{i})^{3}}{p_{i}^{2}}
- \frac{1}{12} \cdot \frac{(\hat p_{i} - p_{i})^{4}}{\{p_{i} + \theta(\hat p_{i} - p_{i})\}^{3}}
\label{ice5}
\end{equation}
\[ 0 < \theta < 1\]

\noindent
By summing both sides of equation \ref{ice5}, we obtain
\[
\sum_{i=1}^{s}\hat p_{i}\ln \hat p_{i} = \hat H = H(\hat p_{1}, \cdots , \hat p_{s})
\]
\begin{equation}
= H(p_{1}, \cdots , p_{s})
- \sum_{i=1}^{s}(\hat p_{i} - p_{i})(1 + \ln p_{i})
- \sum_{i=1}^{s}\frac{1}{2} \cdot \frac{(\hat p_{i} - p_{i})^{2}}{p_{i}}
+ \sum_{i=1}^{s}\frac{1}{6} \cdot \frac{(\hat p_{i} - p_{i})^{3}}{p_{i}^{2}}
- \sum_{i=1}^{s}\frac{1}{12} \cdot \frac{(\hat p_{i} - p_{i})^{4}}{\{p_{i} + \theta(\hat p_{i} - p_{i})\}^{3}}
\end{equation}

\noindent
With some help of equations of the central moments, such as

\[
\mbox{E}(\hat p_{i}) = p_{i}, \quad \mbox{E}((\hat p_{i} - p_{i})^{2}) = \frac{p_{i}(1-p_{i})}{N}
\]

\noindent
we can derive equations such as

\[
\mbox{E}\left(\sum_{i=1}^{s}(\hat p_{i} - p_{i})(1 + \ln p_{i}) \right)
= \mbox{E}\left(\sum_{i=1}^{s}(p_{i} - p_{i})(1 + \ln p_{i}) \right) = 0
\]

\[
 \mbox{E}\left(\sum_{i=1}^{s}\frac{1}{2}\cdot \frac{(\hat p_{i} -
 p_{i})^2}{p_{i}}\right)
= \frac{1}{2}\sum_{i=1}^{s}\mbox{E}
  \left(\frac{(\hat p_{i} - p_{i})^2}{p_{i}}\right)
= \frac{1}{2}\sum_{i=1}^{s}\frac{p_{i}(1-p_{i})}{Np_{i}}
= \frac{s-1}{2N}
\]


\noindent
we are able to obtain

\begin{equation}
\mbox{E}(\hat H) = H - \frac{s-1}{2N} + O\left(\frac{1}{N^{2}}\right)
\label{ice10}
\end{equation}

\noindent
By multiplying both side of \ref{ice10} by \(\log_{2}e\), we obtain \ref{mean_entro}.

\section{Correlation test}
\label{cor_test}

Suppose that \(n\) elements can be divided into \(r\) groups of
category A, and also \(c\) groups of another category B. Let
\(n_{i\cdot}\) be number of elements which belong to group \(i\) of
category A, and \(n_{\cdot j}\) be number of elements which belong to
group \(j\) of category B. And let \(n_{ij}\) be number of observed
elements which belong to group \(i\) of category A and also group
\(j\) of category B.

\[
n_{i\cdot} = \sum_{j=1}^{c}n_{ij}\quad
n_{\cdot j} = \sum_{i=1}^{r}n_{ij}
\]

\begin{equation}
n = \sum_{i=1}^{r}n_{i\cdot} = \sum_{j=1}^{c}n_{\cdot j} 
= \sum_{i=1}^{r}\sum_{j=1}^{c}n_{ij}
\end{equation}

Simultaneously, let \(p_{i\cdot}\) be probability of having an element
which belongs to group \(i\) of category A and let \(p_{\cdot j}\) be
probability of having an element which belongs to group \(j\) of
category B.  And \(p_{ij}\) be probability of having an element which
belongs to both group \(i\) of category A and group \(j\) of category
B.

\[
p_{i\cdot} = \sum_{j=1}^{c}p_{ij}\quad
p_{\cdot j} = \sum_{i=1}^{r}p_{ij}
\]

\begin{equation}
\sum_{i=1}^{r}p_{i\cdot} = \sum_{j=1}^{c}p_{\cdot j} 
= \sum_{i=1}^{r}\sum_{j=1}^{c}p_{ij} = 1
\end{equation}

The likelihood function would be

\begin{equation}
f(N|p) = 
{n\choose{n_{11} \cdots n_{rc}}}p_{11}^{n_{11}}\cdots p_{rc}^{n_{rc}}
\label{fl1}
\end{equation}

To find maximum likelihood estimator for \ref{fl1}, we apply
Lagrange's multiplier method. We define function \(g\) as

\begin{equation}
g(p,\lambda) = \log{n\choose{n_{11} \cdots n_{rc}}} 
             + \sum_{i=1}^{r}\sum_{j=1}^{c}n_{ij}\log p_{ij}
             + \lambda(p_{11} + \cdots + p_{rc} - 1)
\end{equation}

And by solving

\[
\left\{
\begin{array}{l}
\frac{\partial g}{\partial p_{ij}} = \frac{n_{ij}}{p_{ij}} - \lambda = 0\\
\frac{\partial g}{\partial \lambda} = p_{11} + \cdots + p_{rc} - 1 = 0
\end{array}\right.
\]

Since \(\frac{n_{ij}}{p_{ij}}\) must be constant \(\lambda\) for all
\(i\) and \(j\), \(p_{ij}\) must be \(\frac{n_{ij}}{n}\) to have
\(\lambda = n\) (\(n\) is constant). Thus maximum
likelihood estimator \(\hat p_{ij}\) for \ref{fl1} would be 

\[
\hat p_{ij} = \frac{n_{ij}}{n}
\]

Now suppose the null hypothesis 

\[
p_{ij} = q_{ij}
\]

and the alternative hypothesis

\[
p_{ij} \neq q_{ij}
\]

Thus log likelihood ratio statistics \(k\) would be

\begin{equation}
k = 2\log\frac{{\rm max}(f(N|p))}{{\rm max}(f(N|q))}
 = 2\log\frac{{n\choose{n_{11} \cdots n_{rc}}}\left(\frac{n_{11}}{n}\right)^{n_{11}} \cdots \left(\frac{n_{rc}}{n}\right)^{n_{rc}}}
{{n\choose{n_{11} \cdots n_{rc}}}q_{11}^{n_{11}} \cdots q_{rc}^{n_{rc}}}
= 2\sum_{i=1}^{r}\sum_{j=1}^{c}n_{ij}\log\frac{n_{ij}}{nq_{ij}}
\label{fl2}
\end{equation}

Since dim\((p) = (r - 1)(c - 1)\) and dim\((q) = 0\)(\(q\) is fixed),
it can be shown that \(k\) will follow \(\chi^{2}\) distribution with
\((r-1)(c-1)\) degrees of freedom\cite{label5050}.

\[
k \sim \chi^{2}
\]

Let \(P_{i\cdot} = \frac{n_{i}}{n}, 
P_{\cdot j} = \frac{n_{j}}{n}, 
P_{ij} = \frac{n_{ij}}{n}\). \ref{fl2} can be written

\begin{equation}
k = 2n\sum_{i=1}^{r}\sum_{j=1}^{c}P_{ij}\log\frac{P_{ij}}{q_{ij}}
\end{equation}

If we set null hypothesis 

\[
p_{ij} = p_{i\cdot}p(j|i)
\]

then

\begin{equation}
k = 2n\sum_{i=1}^{r}\sum_{j=1}^{c}P_{ij}\log\frac{P_{ij}}{P_{i\cdot}p(j|i)}
\end{equation}

If we set null hypothesis

\[
p_{ij} = p_{i\cdot}p_{\cdot j}
\]

then

\begin{equation}
k = 2n\sum_{i=1}^{r}\sum_{j=1}^{c}P_{ij}\log\frac{P_{ij}}{P_{i\cdot}P_{j\cdot}}
\end{equation}

